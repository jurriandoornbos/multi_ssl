{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c52e8e-1466-400a-b890-63dbee42d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from multissl.data import get_transform, tifffile_loader, MixedUAVDataset\n",
    "import tifffile as tiff\n",
    "from multissl.models import build_model\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Dict, Optional, Tuple, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d704a2f2-cd91-4fc1-a628-12ed5066c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset, dataset size: 480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def restack_nested(list_of_tensor_lists):\n",
    "    #     Determine the number of tensor positions from the first list\n",
    "    num_positions = len(list_of_tensor_lists[0])\n",
    "    \n",
    "    # Initialize empty lists to collect tensors at each position\n",
    "    collected_tensors = [[] for _ in range(num_positions)]\n",
    "    \n",
    "    # Collect tensors by their position in each inner list\n",
    "    for tensor_list in list_of_tensor_lists:\n",
    "        for i, tensor in enumerate(tensor_list):\n",
    "            collected_tensors[i].append(tensor)\n",
    "    \n",
    "    # Stack each collection of tensors\n",
    "    return [torch.stack(tensor_collection) for tensor_collection in collected_tensors]\n",
    "    \n",
    "\n",
    "\n",
    "def multisensor_views_collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, Any]:\n",
    "    \"\"\"Collect samples by type for separate processing\"\"\"\n",
    "    result = {\n",
    "        'rgb_only': {'data': [], 'indices': []},\n",
    "        'ms_only': {'data': [], 'indices': []},\n",
    "        'aligned': {'rgb': [], 'ms': [], 'indices': []}\n",
    "    }\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        if item['type'] == 'rgb_only':\n",
    "            result['rgb_only']['data'].append(item['rgb'])\n",
    "            result['rgb_only']['indices'].append(i)\n",
    "        elif item['type'] == 'ms_only':\n",
    "            result['ms_only']['data'].append(item['ms'])\n",
    "            result['ms_only']['indices'].append(i)\n",
    "        elif item['type'] == 'aligned':\n",
    "            result['aligned']['rgb'].append(item['rgb'])\n",
    "            result['aligned']['ms'].append(item['ms'])\n",
    "            result['aligned']['indices'].append(i)\n",
    "    \n",
    "    # Convert lists to tensors where applicable\n",
    "    if result['rgb_only']['data']:\n",
    "        result['rgb_only']['data'] = restack_nested(result['rgb_only']['data'])\n",
    "    if result['ms_only']['data']:\n",
    "        result['ms_only']['data'] = restack_nested(result['ms_only']['data'])\n",
    "    if result['aligned']['rgb']:\n",
    "        result['aligned']['rgb'] = restack_nested(result['aligned']['rgb'])\n",
    "        result['aligned']['ms'] = restack_nested(result['aligned']['ms'])\n",
    "    \n",
    "    # Add original batch size for reference\n",
    "    result['batch_size'] = len(batch)\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "# Create a multiview transform that returns three different augmentations of each image.\n",
    "transform_multispectral = get_transform(img_size=224, std_noise  =0.1, \n",
    "                                        brightness_factor=0.1 ,\n",
    "                                        max_shift=0.1)\n",
    "tfs = [transform_multispectral for i in range(4)]\n",
    "\n",
    "transform_ms = MultiViewTransform(transforms=tfs)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "\n",
    "dataset_train_msrgb = MixedUAVDataset(\n",
    "    root_dir =\"../../msdata/data/output_multi/\" ,\n",
    "    transform = transform_ms,\n",
    "    reduce_by = 160\n",
    ")\n",
    "\n",
    "length_dataset = len(dataset_train_msrgb)\n",
    "print(\"Loaded dataset, dataset size: \"+ str(length_dataset))\n",
    "\n",
    "\n",
    "dataloader_train_msrgb = torch.utils.data.DataLoader(\n",
    "    dataset_train_msrgb,                            # Pass the dataset to the dataloader.\n",
    "    batch_size=16,         # A large batch size helps with learning.\n",
    "    shuffle=True,                       # Shuffling is important!\n",
    "    drop_last = True,\n",
    "    num_workers=0,\n",
    "    collate_fn = multisensor_views_collate_fn\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ffbd63-50fe-4f14-88ff-43d3482ec414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train_msrgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82a4ba7-f083-4b49-9259-c8205aa7ec99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"rgb_only\"][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e942660-d9bd-4fb7-b59f-605b0de6ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset, dataset size: 116608\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from your image folder.\n",
    "dataset_train_ms = LightlyDataset(\n",
    "    input_dir = \"../../msdata/data/output_multi/RGBMS\",\n",
    "    transform = transform_ms,\n",
    ")\n",
    "\n",
    "def jpg_loader(f):\n",
    "    return np.array(Image.open(f))\n",
    "def tifffile_loader(f):\n",
    "    img_path = f\n",
    "    with tiff.TiffFile(img_path) as tif:\n",
    "        image_array = tif.asarray()\n",
    "        print(f\"Array shape: {image_array.shape}\")\n",
    "    return image_array\n",
    "dataset_train_ms.dataset.loader = tifffile_loader\n",
    "\n",
    "length_dataset = len(dataset_train_ms)\n",
    "print(\"Loaded dataset, dataset size: \"+ str(length_dataset))\n",
    "    \n",
    "# Build a PyTorch dataloader.\n",
    "dataloader_train_ms = torch.utils.data.DataLoader(\n",
    "    dataset_train_ms,                            # Pass the dataset to the dataloader.\n",
    "    batch_size=16,         # A large batch size helps with learning.\n",
    "    shuffle=True,                       # Shuffling is important!\n",
    "    drop_last = True,\n",
    "    num_workers=0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94621638-a506-4b62-a60c-d96ce4a015a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 3)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 3)\n",
      "Array shape: (512, 512, 3)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 3)\n",
      "Array shape: (512, 512, 4)\n",
      "Array shape: (512, 512, 4)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 224, 224] at entry 0 and [3, 224, 224] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m batch2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_train_ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:223\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    221\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transposed):\n\u001b[1;32m--> 223\u001b[0m         clone[i] \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 224, 224] at entry 0 and [3, 224, 224] at entry 3"
     ]
    }
   ],
   "source": [
    "batch2 = next(iter(dataloader_train_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebd7ad22-95fe-49ab-911a-924b82ada99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946948c-3670-463b-9b4a-e434fd2ac05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
