{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6470e28d-5790-41ab-a209-15be3de6ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judoj\\mambaforge\\envs\\lightly\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from lightly.data.dataset import LightlyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d98ec6-22bd-4de1-96a7-40e61393f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multissl.models import MSRGBInstanceModule\n",
    "from multissl.plotting.rgb_batch import rgb_visualize_batch\n",
    "from multissl.data.instance_segmentation_dataset import COCOInstanceSegmentationDataset, get_instance_transforms,instance_segmentation_collate_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e9ece94-657f-4064-b9f0-ee429fc92c4f",
   "metadata": {},
   "source": [
    "# Segmentation Head Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b76fdb2-a1e3-439f-9666-d0ca181698ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../checkpoints_convnext_tiny/last.ckpt\n",
      "Unexpected keys: ['projection_head.layers.0.weight', 'projection_head.layers.1.weight', 'projection_head.layers.1.bias', 'projection_head.layers.1.running_mean', 'projection_head.layers.1.running_var', 'projection_head.layers.1.num_batches_tracked', 'projection_head.layers.3.weight', 'projection_head.layers.4.weight', 'projection_head.layers.4.bias', 'projection_head.layers.4.running_mean', 'projection_head.layers.4.running_var', 'projection_head.layers.4.num_batches_tracked', 'projection_head.layers.6.weight', 'projection_head.layers.7.running_mean', 'projection_head.layers.7.running_var', 'projection_head.layers.7.num_batches_tracked', 'prediction_head.layers.0.weight', 'prediction_head.layers.1.weight', 'prediction_head.layers.1.bias', 'prediction_head.layers.1.running_mean', 'prediction_head.layers.1.running_var', 'prediction_head.layers.1.num_batches_tracked', 'prediction_head.layers.3.weight', 'prediction_head.layers.3.bias']\n",
      "{'layer1': 96, 'layer2': 192, 'layer3': 384, 'layer4': 768}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = {\"checkpoint_path\":\"../checkpoints_convnext_tiny/last.ckpt\",\n",
    "    \"num_classes\": 2,\n",
    "    \"class_names\": [\"Background\", \"Solarpanel\"],\n",
    "    \"freeze_backbone\": False,\n",
    "    \"batch_size\": 8,\n",
    "    \"img_size\": 448,\n",
    "\n",
    "       }\n",
    "# pretrained tiny has hierarchical fusion: at every layer MS +RGB is fused with attention\n",
    "\n",
    "pl_model =  MSRGBInstanceModule(\n",
    "        num_classes=args[\"num_classes\"],  # Binary segmentation (background, foreground)\n",
    "        rgb_in_channels=3,\n",
    "        ms_in_channels=5,  # Adjust based on your MS data\n",
    "        model_size='tiny',  # Can be 'tiny', 'small', 'base', 'large'\n",
    "        fusion_strategy='hierarchical',  # 'early', 'late', 'hierarchical', 'progressive'\n",
    "        fusion_type='attention',  # 'concat', 'add', 'attention'\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        pretrained_backbone=args[\"checkpoint_path\"],  # Path to pretrained weights if available\n",
    "        freeze_backbone = args[\"freeze_backbone\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64fdfd0-986f-46ca-8b87-22721d1bd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../dataset/solarcoco/imgs\"\n",
    "mask_path = \"../dataset/solarcoco/annotations/frame_000003.JSON\"\n",
    "instance_path = \"../dataset/solarcoco/instance\"\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = COCOInstanceSegmentationDataset(\n",
    "   coco_json_path = mask_path,\n",
    "    img_dir = image_path,\n",
    "    instance_dir = instance_path,\n",
    "    transform=get_instance_transforms(img_size=args[\"img_size\"], augment=True)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890e8f92-abf5-4d52-abb7-324f61cbf02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RepeatingBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that repeats dataset indices to ensure each batch contains batch_size items.\n",
    "    Useful when working with small datasets or when you want to apply heavy augmentation\n",
    "    to a small set of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_size, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_size: Number of samples in the original dataset\n",
    "            batch_size: Desired batch size \n",
    "            shuffle: Whether to shuffle the data or access sequentially\n",
    "        \"\"\"\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Number of copies needed for each dataset item\n",
    "        self.copies_per_item = max(1, batch_size // dataset_size)\n",
    "        \n",
    "        # Extra samples needed beyond perfect division\n",
    "        self.extra_samples = batch_size - (dataset_size * self.copies_per_item)\n",
    "        if self.extra_samples < 0:\n",
    "            self.extra_samples = 0\n",
    "        \n",
    "        # Total number of indices we'll generate\n",
    "        self.total_samples = dataset_size * self.copies_per_item + self.extra_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Create base indices\n",
    "        if self.shuffle:\n",
    "            # For each \"epoch\", we shuffle the dataset order\n",
    "            base_indices = torch.randperm(self.dataset_size).tolist()\n",
    "        else:\n",
    "            base_indices = list(range(self.dataset_size))\n",
    "        \n",
    "        # Repeat each index the required number of times\n",
    "        repeated_indices = []\n",
    "        for idx in base_indices:\n",
    "            repeated_indices.extend([idx] * self.copies_per_item)\n",
    "        \n",
    "        # Add extra samples if needed to exactly reach batch_size\n",
    "        if self.extra_samples > 0:\n",
    "            extra_indices = base_indices[:self.extra_samples]\n",
    "            repeated_indices.extend(extra_indices)\n",
    "        \n",
    "        # Shuffle the final indices to mix different samples\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(repeated_indices)\n",
    "        \n",
    "        return iter(repeated_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af897d4-9928-44ae-b253-239a51636baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random\n",
    "\n",
    "def plot_batch_item(batch_item, figsize=(15, 5), show_masks=True, show_boxes=True, \n",
    "                   mask_alpha=0.3, box_linewidth=4):\n",
    "    \"\"\"\n",
    "    Plot a single item from a batch containing:\n",
    "    - rgb: RGB image tensor (3, H, W)\n",
    "    - boxes: List of bounding boxes in format [confidence, cx, cy, w, h] (normalized)\n",
    "    - instance_masks: List of instance segmentation masks\n",
    "    - instance_classes: Class labels for each instance\n",
    "    - mask: Semantic segmentation mask\n",
    "    \n",
    "    Args:\n",
    "        batch_item: Dictionary with keys 'rgb', 'boxes', 'instance_masks', 'instance_classes', 'mask'\n",
    "        figsize: Figure size tuple\n",
    "        show_masks: Whether to show instance masks\n",
    "        show_boxes: Whether to show bounding boxes\n",
    "        mask_alpha: Transparency for mask overlay\n",
    "        box_linewidth: Line width for bounding boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    rgb_tensor = batch_item['rgb']\n",
    "    boxes = batch_item['boxes']\n",
    "    instance_masks = batch_item['instance_masks']\n",
    "    instance_classes = batch_item['instance_classes']\n",
    "    semantic_mask = batch_item['mask']\n",
    "    \n",
    "    # Convert RGB tensor to numpy (H, W, 3)\n",
    "    if isinstance(rgb_tensor, torch.Tensor):\n",
    "        rgb_img = rgb_tensor.permute(1, 2, 0).numpy()\n",
    "    else:\n",
    "        rgb_img = rgb_tensor\n",
    "    \n",
    "    # Ensure RGB values are in [0, 1] range\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = rgb_img.shape[:2]\n",
    "    \n",
    "    # Create subplots\n",
    "    n_plots = 3 if show_masks else 2\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=figsize)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot 1: Original RGB image with bounding boxes\n",
    "    axes[0].imshow(rgb_img)\n",
    "    axes[0].set_title('RGB Image with Bounding Boxes')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    if show_boxes:\n",
    "        # Generate colors for different instances\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(boxes)))\n",
    "        \n",
    "        for i, (box, class_id) in enumerate(zip(boxes, instance_classes)):\n",
    "            if isinstance(box, torch.Tensor):\n",
    "                box = box.numpy()\n",
    "            if isinstance(class_id, torch.Tensor):\n",
    "                class_id = class_id.item()\n",
    "            \n",
    "            # Convert from (confidence, cx, cy, w, h) to pixel coordinates\n",
    "            confidence = box[0]\n",
    "            cx, cy, w, h = box[1:]\n",
    "            \n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            x_center = cx * width\n",
    "            y_center = cy * height\n",
    "            box_width = w * width\n",
    "            box_height = h * height\n",
    "            \n",
    "            # Convert center coordinates to top-left coordinates\n",
    "            x_left = x_center - box_width / 2\n",
    "            y_top = y_center - box_height / 2\n",
    "            \n",
    "            # Create rectangle patch\n",
    "            rect = patches.Rectangle(\n",
    "                (x_left, y_top), box_width, box_height,\n",
    "                linewidth=box_linewidth, edgecolor=colors[i], \n",
    "                facecolor='none', alpha=0.8\n",
    "            )\n",
    "            axes[0].add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            axes[0].text(x_left, y_top - 5, f'Class {class_id} ({confidence:.2f})',\n",
    "                        color=colors[i], fontsize=10, fontweight='bold',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Plot 2: Semantic segmentation mask\n",
    "    axes[1].imshow(semantic_mask, cmap='viridis')\n",
    "    axes[1].set_title('Semantic Segmentation Mask')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot 3: Instance masks overlay (if requested)\n",
    "    if show_masks and len(axes) > 2:\n",
    "        # Create overlay image\n",
    "        overlay_img = rgb_img.copy()\n",
    "        \n",
    "        # Generate colors for different instances\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(instance_masks)))\n",
    "        \n",
    "        for i, mask in enumerate(instance_masks):\n",
    "            mask = mask==1\n",
    "            if isinstance(mask, torch.Tensor):\n",
    "                mask = mask.numpy()\n",
    "            \n",
    "            # Create colored mask\n",
    "            color_mask = np.zeros_like(rgb_img)\n",
    "            color_mask[mask] = colors[i][:3]  # Use RGB components\n",
    "            \n",
    "            # Blend with original image\n",
    "            mask_area = mask[..., np.newaxis]\n",
    "            overlay_img = overlay_img * (1 - mask_area * mask_alpha) + color_mask * mask_area * mask_alpha\n",
    "        \n",
    "        axes[2].imshow(np.clip(overlay_img, 0, 1))\n",
    "        axes[2].set_title('RGB Image with Instance Masks')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def plot_batch(batch, max_items=4, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot multiple items from a batch.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of batch items or single batch item\n",
    "        max_items: Maximum number of items to plot\n",
    "        **kwargs: Additional arguments passed to plot_batch_item\n",
    "    \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        # Single item\n",
    "        return plot_batch_item(batch, **kwargs)\n",
    "    \n",
    "    # Multiple items\n",
    "    n_items = min(len(batch), max_items)\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        print(f\"\\nPlotting batch item {i}:\")\n",
    "        plot_batch_item(batch[i], **kwargs)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2400c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RepeatingBatchSampler(dataset_size=len(dataset),batch_size=args[\"batch_size\"],\n",
    "    shuffle=True\n",
    ")\n",
    "collate_fn = instance_segmentation_collate_fn\n",
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=args[\"batch_size\"], \n",
    "    num_workers=0,\n",
    "    # Use 0 for single image to avoid overhead,\n",
    "    sampler = sampler,\n",
    "    collate_fn = collate_fn,\n",
    "    drop_last= True\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9109e1cd-33ef-46ad-bd68-30cbda298308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\judoj\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "C:\\Users\\judoj\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ backbone  │ MSRGBConvNeXtFeatureExtractor │ 85.3 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ fpn       │ FeaturePyramidNetwork         │  2.7 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ rpn       │ BatchedRegionProposalNetwork  │  593 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ roi_heads │ RoIHeads                      │ 16.6 M │ train │\n",
       "└───┴───────────┴───────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ backbone  │ MSRGBConvNeXtFeatureExtractor │ 85.3 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ fpn       │ FeaturePyramidNetwork         │  2.7 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ rpn       │ BatchedRegionProposalNetwork  │  593 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ roi_heads │ RoIHeads                      │ 16.6 M │ train │\n",
       "└───┴───────────┴───────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 105 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 105 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 420                                                                        \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 506                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 105 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 105 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 420                                                                        \n",
       "\u001b[1mModules in train mode\u001b[0m: 506                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b126c6f23f47f4afd9b5c3bec9d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judoj\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         closure()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    174\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1273\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 223\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    226\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m     optimizer: Steppable,\n\u001b[0;32m    100\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    101\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\judoj\\documents\\programming\\multi_ssl\\multissl\\models\\msrgb_convnext_instance.py:986\u001b[0m, in \u001b[0;36mMSRGBInstanceModule.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    984\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(target)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# forward and compute losses\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mms_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrgb_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# log losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\judoj\\documents\\programming\\multi_ssl\\multissl\\models\\msrgb_convnext_instance.py:917\u001b[0m, in \u001b[0;36mMSRGBInstanceModule.forward\u001b[1;34m(self, rgb, ms, targets)\u001b[0m\n\u001b[0;32m    915\u001b[0m     image_sizes \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m rgb]\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;66;03m# Run RPN on FPN features\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m     proposals, rpn_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     image_sizes \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m ms]\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\lightly\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\users\\judoj\\documents\\programming\\multi_ssl\\multissl\\models\\msrgb_convnext_instance.py:458\u001b[0m, in \u001b[0;36mBatchedRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# Convert features dict to list for compatibility\u001b[39;00m\n\u001b[0;32m    457\u001b[0m feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 458\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Generate anchors\u001b[39;00m\n\u001b[0;32m    460\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(images, feature_maps, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# Create progress bar callback\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "class LossProgressBar(RichProgressBar):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        super().on_train_epoch_end(trainer, pl_module)\n",
    "        loss = float(trainer.callback_metrics.get('train_total_loss', 0))\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "progress_bar = LossProgressBar()\n",
    "\n",
    "# Create model checkpoint callback\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='train_total_loss',\n",
    "    filename='pasiphae-upernet-{epoch:02d}-{train_total_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='train_total_loss',\n",
    "    patience=50,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    callbacks=[progress_bar, checkpoint_callback, early_stop_callback],\n",
    "    logger=True,\n",
    "    log_every_n_steps=1,\n",
    "    accelerator = \"cuda\"\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(pl_model, dataloader)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe1a0f-dcc2-411e-abbe-a9648a504fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "losses = progress_bar.losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9ce60-b1b2-41b9-a38e-b352ca74b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset:\n",
    "# Create train/val/test datasets\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((args[\"img_size\"],args[\"img_size\"] ), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "solar_folder = \"../dataset/pvhawk\" \n",
    "\n",
    "non_labeled = LightlyDataset(input_dir = \"../dataset/pvhawk\",transform=rgb_transform)\n",
    "non_labeled = torch.utils.data.DataLoader(non_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633114a-9d2e-45be-b44f-eb2841ad9956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process all images\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda'\n",
    "pl_model.to(device)\n",
    "all_results = []\n",
    "\n",
    "pl_model.eval()\n",
    "\n",
    "for rgb, idx, img_name in tqdm(iter(non_labeled)):\n",
    "    rgb = rgb.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = pl_model(rgb  = rgb)\n",
    "        # Get semantic segmentation prediction\n",
    "    pred_mask = torch.argmax(outputs[\"sem_logits\"], dim=1).cpu().numpy()\n",
    "    outputs =   pl_model.predict_instances(\n",
    "                outputs, \n",
    "                confidence_threshold=0.5\n",
    "            )\n",
    "\n",
    "    # Get instance masks if available\n",
    "    if 'instance_masks' in outputs:\n",
    "\n",
    "        # If mask_coeffs are already in mask format\n",
    "        instance_masks = [output.cpu().numpy() for output in outputs['instance_masks']]\n",
    "        \n",
    "    else:\n",
    "        instance_masks = None\n",
    "    \n",
    "    # Get instance boxes if available\n",
    "    instance_boxes = outputs.get('boxes', None)\n",
    "    if instance_boxes is not None:\n",
    "        instance_boxes = instance_boxes.cpu().numpy()\n",
    "    \n",
    "    # Get instance classes if available\n",
    "    instance_classes = outputs.get('cls_scores', None)\n",
    "    if instance_classes is not None:\n",
    "        # Get the predicted class for each instance\n",
    "        if instance_classes.dim() > 2:  # If shape is [B, N, C]\n",
    "            instance_class_ids = torch.argmax(instance_classes, dim=2).cpu().numpy()\n",
    "        else:\n",
    "            instance_class_ids = torch.argmax(instance_classes, dim=1).cpu().numpy()\n",
    "    else:\n",
    "        instance_class_ids = None\n",
    "    \n",
    "    # Get center heatmap if available\n",
    "    if 'center_heatmap' in outputs:\n",
    "        center_heatmap = outputs['center_heatmap'].cpu().numpy()\n",
    "    else:\n",
    "        center_heatmap = None\n",
    "    \n",
    "    # Convert tensor back to PIL image for visualization\n",
    "    orig_img = rgb.cpu().squeeze(0)\n",
    "    # Denormalize if needed\n",
    "    orig_img = torch.clamp(orig_img, 0, 1)\n",
    "    \n",
    "    # Store all results together\n",
    "    all_results.append({\n",
    "        'image': orig_img.permute(1, 2, 0).numpy(),  # Convert to HWC format for visualization\n",
    "        'mask': pred_mask[0],  # Semantic segmentation mask\n",
    "        'instance_masks': instance_masks[0] if instance_masks is not None else None,  # Instance masks\n",
    "        'instance_boxes': instance_boxes[0] if instance_boxes is not None else None,  # Bounding boxes\n",
    "        'instance_classes': instance_class_ids[0] if instance_class_ids is not None else None,  # Class IDs\n",
    "        'center_heatmap': center_heatmap[0] if center_heatmap is not None else None,  # Center points\n",
    "        'filename': img_name[0] if isinstance(img_name, list) else img_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2649-52e0-4afb-ab96-44f85fa5d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=10\n",
    "import random\n",
    "# Sample random images\n",
    "if len(all_results) > num_samples:\n",
    "    random_samples = random.sample(all_results, num_samples)\n",
    "else:\n",
    "    random_samples = all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3d4f8-5e79-4e63-80fe-1f67075d1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "def visualize_segmentation(samples, save_path='segmentation_results.png'):\n",
    "    \"\"\"\n",
    "    Visualize segmentation results including original image, semantic segmentation mask,\n",
    "    instance masks, and bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of dictionaries containing results for each sample\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    # Determine the number of columns - add columns for instance segmentation if available\n",
    "    num_cols = 2  # Default: Original image + semantic mask\n",
    "    \n",
    "    # Check if we have instance masks\n",
    "    has_instances = any(sample.get('instance_masks') is not None for sample in samples)\n",
    "    has_boxes = any(sample.get('instance_boxes') is not None for sample in samples)\n",
    "    has_centers = any(sample.get('center_heatmap') is not None for sample in samples)\n",
    "    \n",
    "    if has_instances:\n",
    "        num_cols += 1  # Add column for instance masks\n",
    "    if has_boxes:\n",
    "        num_cols += 1  # Add column for box visualization\n",
    "    if has_centers:\n",
    "        num_cols += 1  # Add column for center heatmap\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(len(samples), num_cols, figsize=(4*num_cols, 4*len(samples)))\n",
    "    \n",
    "    # If only one sample, wrap axes in a list\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes] if num_cols == 1 else [axes]\n",
    "    \n",
    "    # Create colormap for instances\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib as mpl\n",
    "    \n",
    "    # Generate distinct colors for instance visualization\n",
    "    num_colors = 20  # Arbitrary, could be increased\n",
    "    cmap = plt.cm.get_cmap('tab20', num_colors)\n",
    "    colors = [cmap(i) for i in range(num_colors)]\n",
    "    \n",
    "    # Process each sample\n",
    "    for i, sample in enumerate(samples):\n",
    "        col_idx = 0\n",
    "        \n",
    "        # Original image\n",
    "        axes[i][col_idx].imshow(sample['image'])\n",
    "        #axes[i][col_idx].set_title(f\"Original: {os.path.basename(sample['filename'])}\")\n",
    "        axes[i][col_idx].axis('off')\n",
    "        col_idx += 1\n",
    "        \n",
    "        # Semantic segmentation mask\n",
    "        sem_mask = sample['mask']\n",
    "        num_classes = len(np.unique(sem_mask))\n",
    "        \n",
    "        # Create a more visually distinct colormap for semantic classes\n",
    "        if num_classes <= 2:  # Binary mask\n",
    "            axes[i][col_idx].imshow(sem_mask, cmap='viridis')\n",
    "        else:  # Multi-class mask\n",
    "            axes[i][col_idx].imshow(sem_mask, cmap='nipy_spectral')\n",
    "            \n",
    "        axes[i][col_idx].set_title(f\"Semantic Mask\")\n",
    "        axes[i][col_idx].axis('off')\n",
    "        col_idx += 1\n",
    "        \n",
    "        # Instance segmentation visualization\n",
    "        if has_instances and sample.get('instance_masks') is not None:\n",
    "            instance_masks = sample['instance_masks']\n",
    "            num_instances = instance_masks.shape[0]\n",
    "            \n",
    "            # Create a visualization with each instance in a different color\n",
    "            instance_vis = np.zeros((*sem_mask.shape, 3), dtype=np.float32)\n",
    "            \n",
    "            for inst_idx in range(num_instances):\n",
    "                # Get color for this instance\n",
    "                color = colors[inst_idx % num_colors][:3]\n",
    "                \n",
    "                # Create mask for this instance (threshold at 0.5 if not binary)\n",
    "                mask = instance_masks[inst_idx] > 0.5 if isinstance(instance_masks[inst_idx].max(), (int, float)) else instance_masks[inst_idx] > 0\n",
    "                \n",
    "                # Skip empty masks\n",
    "                if not np.any(mask):\n",
    "                    continue\n",
    "                \n",
    "                # Add this instance with its color\n",
    "                for c in range(3):\n",
    "                    instance_vis[:, :, c] = np.where(mask, \n",
    "                                                   color[c], \n",
    "                                                   instance_vis[:, :, c])\n",
    "            \n",
    "            # Overlay instances on the image with alpha blending\n",
    "            overlay = sample['image'].copy()\n",
    "            alpha = 0.7  # Transparency for the instance masks\n",
    "            \n",
    "            # Only apply alpha blending where masks exist\n",
    "            any_mask = np.any(instance_vis > 0, axis=2)\n",
    "            for c in range(3):\n",
    "                overlay[:, :, c] = np.where(any_mask, \n",
    "                                          overlay[:, :, c] * (1 - alpha) + instance_vis[:, :, c] * alpha,\n",
    "                                          overlay[:, :, c])\n",
    "            \n",
    "            axes[i][col_idx].imshow(overlay)\n",
    "            axes[i][col_idx].set_title(f\"Instance Masks ({num_instances} instances)\")\n",
    "            axes[i][col_idx].axis('off')\n",
    "            col_idx += 1\n",
    "        \n",
    "        # Bounding box visualization\n",
    "        if has_boxes and sample.get('instance_boxes') is not None:\n",
    "            boxes = sample['instance_boxes']\n",
    "            \n",
    "            # Create a copy of the original image for box visualization\n",
    "            box_vis = sample['image'].copy()\n",
    "            \n",
    "            # Draw rectangles\n",
    "            from matplotlib.patches import Rectangle\n",
    "            \n",
    "            # Display the image\n",
    "            axes[i][col_idx].imshow(box_vis)\n",
    "            \n",
    "            # Draw each box\n",
    "            for box_idx, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = box\n",
    "                \n",
    "                # If boxes are normalized to 0-1, scale to image dimensions\n",
    "                if x1 <= 1.0 and y1 <= 1.0 and x2 <= 1.0 and y2 <= 1.0:\n",
    "                    h, w = box_vis.shape[:2]\n",
    "                    x1, y1, x2, y2 = x1 * w, y1 * h, x2 * w, y2 * h\n",
    "                \n",
    "                # Get color for this box\n",
    "                color = colors[box_idx % num_colors]\n",
    "                \n",
    "                # Create rectangle\n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                linewidth=2, edgecolor=color, facecolor='none')\n",
    "                axes[i][col_idx].add_patch(rect)\n",
    "                \n",
    "                # Add class label if available\n",
    "                if sample.get('instance_classes') is not None:\n",
    "                    class_id = sample['instance_classes'][box_idx]\n",
    "                    axes[i][col_idx].text(x1, y1-5, f\"Class {class_id}\", \n",
    "                                        color=color, fontsize=10, weight='bold')\n",
    "            \n",
    "            axes[i][col_idx].set_title(f\"Bounding Boxes\")\n",
    "            axes[i][col_idx].axis('off')\n",
    "            col_idx += 1\n",
    "        \n",
    "        # Center heatmap visualization\n",
    "        if has_centers and sample.get('center_heatmap') is not None:\n",
    "            heatmap = sample['center_heatmap']\n",
    "            \n",
    "            # Handle different heatmap formats\n",
    "            if heatmap.ndim > 2:\n",
    "                if heatmap.shape[0] == 1:  # [1, H, W]\n",
    "                    heatmap = heatmap[0]\n",
    "                elif heatmap.ndim == 4 and heatmap.shape[0] == 1 and heatmap.shape[1] == 1:  # [1, 1, H, W]\n",
    "                    heatmap = heatmap[0, 0]\n",
    "            \n",
    "            # Normalize heatmap for visualization\n",
    "            if heatmap.max() > 0:\n",
    "                heatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "            else:\n",
    "                heatmap_norm = heatmap\n",
    "            \n",
    "            # Show heatmap with a red-yellow colormap\n",
    "            axes[i][col_idx].imshow(heatmap_norm, cmap='hot')\n",
    "            axes[i][col_idx].set_title(\"Center Heatmap\")\n",
    "            axes[i][col_idx].axis('off')\n",
    "            col_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ##plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "visualize_segmentation(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204e2a1-416c-4d14-b9ff-85cca858d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_path = \"model_trained_on_single_image.pth\"\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print(f\"Model saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
